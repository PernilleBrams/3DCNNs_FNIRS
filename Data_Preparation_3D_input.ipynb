{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation: Stacking the GAF-images to create a 3D input for the CNN\n",
    "When running this code, 3D arrays for each participant for each epoch are made. Depending on the investigation of contrast, the code is run with appropriate active labels in the event dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and packages \n",
    "import sys\n",
    "import numba\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "from os import mkdir\n",
    "import pyvista\n",
    "import pyvistaqt\n",
    "import pandas as pd\n",
    "from itertools import compress\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying paths and creating participant list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying path to participant-folders with epoch-folders containing the GAF images\n",
    "dir_path = \"/path/to/GAF/images\" #\n",
    "\n",
    "# TODO: Specify path to folder where you want to store your .npy files for 1) Epoch_cube_list and 2) Epoch_cube_labels for Participant_i\n",
    "dir_path_save = \"/path/to/folder/for/saving/the/input\"\n",
    "\n",
    "# -- 1) Making a list of participants\n",
    "participant_list = glob.glob(f\"{dir_path}/*/\")\n",
    "if '.DS_Store' in participant_list:\n",
    "    participant_list.remove('.DS_Store')\n",
    "\n",
    "print(participant_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creatin the event dictionary\n",
    "Changes are made here in regard to the desired contrast. The followng code show active labels for the Reflection/Verbalization task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dict_keep_for_stimpres_speak_cont = {\n",
    "    # -- PERCEPTION\n",
    "\n",
    "    # Condition_1\n",
    "                # 'Non-human simple presentation Condition_1 1': 43,\n",
    "                # 'Non-human simple presentation Condition_1 2': 44,\n",
    "                # 'Non-human simple presentation Condition_1 3': 45,\n",
    "\n",
    "                # 'Human simple presentation Condition_1 1': 61,\n",
    "                # 'Human simple presentation Condition_1 2': 62,\n",
    "                # 'Human simple presentation Condition_1 3': 63,\n",
    "\n",
    "    # Condition_2\n",
    "                # 'Non-human simple presentation Condition_2 1': 52,\n",
    "                # 'Non-human simple presentation Condition_2 2': 53,\n",
    "                # 'Non-human simple presentation Condition_2 3': 54,\n",
    "\n",
    "                # 'Human simple presentation Condition_2 1': 70,\n",
    "                # 'Human simple presentation Condition_2 2': 71,\n",
    "                # 'Human simple presentation Condition_2 3': 72,\n",
    "\n",
    "    # -- REFLECTION --\n",
    "    \n",
    "    # Condition_1\n",
    "                # 'Non-human simple think Condition_1 1': 46, \n",
    "                # 'Non-human simple think Condition_1 2': 47,\n",
    "                # 'Non-human simple think Condition_1 3': 48,\n",
    "\n",
    "                # 'Human simple think Condition_1 1': 64,             \n",
    "                # 'Human simple think Condition_1 2': 65,             \n",
    "                # 'Human simple think Condition_1 3': 66,     \n",
    "    \n",
    "    # Condition_2\n",
    "\n",
    "                'Non-human simple think Condition_2 1': 55, \n",
    "                'Non-human simple think Condition_2 2':  56,\n",
    "                'Non-human simple think Condition_2 3': 57,\n",
    "\n",
    "                'Human simple think Condition_2 1': 73, \n",
    "                'Human simple think Condition_2 2': 74,\n",
    "                'Human simple think Condition_2 3': 75,\n",
    "\n",
    "                \n",
    "    # -- VERBALIZATION --\n",
    "\n",
    "    # Condition_1\n",
    "                # 'Non-human simple speak Condition_1 1': 49,\n",
    "                # 'Non-human simple speak Condition_1 2':  50,\n",
    "                # 'Non-human simple speak Condition_1 3': 51,\n",
    "\n",
    "                # 'Human simple speak Condition_1 1': 67,             \n",
    "                # 'Human simple speak Condition_1 2': 68,             \n",
    "                # 'Human simple speak Condition_1 3': 69,  \n",
    "    \n",
    "    # Condition_2\n",
    "\n",
    "                'Non-human simple speak Condition_2 1': 58,\n",
    "                'Non-human simple speak Condition_2 2': 59,\n",
    "                'Non-human simple speak Condition_2 3': 60,\n",
    "\n",
    "                'Human simple speak Condition_2 1': 76,\n",
    "                'Human simple speak Condition_2 2': 77,\n",
    "                'Human simple speak Condition_2 3': 78\n",
    "\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the 2D GAF images for each epoch into a 3D array and save it as an .npy file along with the corresponding labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for participant in participant_list[1:]:\n",
    "    epoch_cube_list_3D = [] # 3D array \n",
    "    epoch_cube_labels_3D = [] # Corresponding label array\n",
    "    epochfolder_list = glob.glob(f\"{participant}/*/\")\n",
    "\n",
    "    for epoch in epochfolder_list:  \n",
    "\n",
    "        if \"_\" not in (epoch)[-3:]: # Only include relevant triggers\n",
    "            if int(epoch[-3:].replace('/','')) in event_dict_keep_for_stimpres_speak_cont.values():\n",
    "                \n",
    "                # Create list of labels\n",
    "                epoch_cube_labels_3D.append(int(epoch[-3:].replace('/',''))) \n",
    "                \n",
    "                os.chdir(epoch) # enter the folder containing the GAF images and retrieve the .png's\n",
    "                images = glob.glob(\"*.png\")\n",
    "                images.sort(key = lambda x: int(x.split(\"-\")[0]))\n",
    "\n",
    "                # Getting the first GAF image as a base for the concatenated image:\n",
    "                im = cv2.imread(images[0])\n",
    "                im = np.asarray(im)\n",
    "\n",
    "                # Give it an extra dimension (needed for modelling)\n",
    "                im = np.reshape(im, (36,36,1,3)) # 2D (h,w,channel) 3D (h,w,d,ch) \n",
    "\n",
    "                # Stack the GAF arrays behind eachother in accordance with spatial channel placement\n",
    "                for png_file in images[1:]:\n",
    "                    im_reshaped = cv2.imread(png_file)\n",
    "                    im_reshaped = np.asarray(im_reshaped)\n",
    "\n",
    "                    # Give it an extra dimension (needed for modelling)\n",
    "                    im_reshaped = np.reshape(im_reshaped, (36,36,1,3))\n",
    "                    \n",
    "                    # Stack the image behind the others\n",
    "                    im = np.concatenate((im, im_reshaped), axis = 2)\n",
    "\n",
    "                #When it's done with the epoch, append im to a list. Should be just as long as the label-list, and should be a list / array of 36,36,3,70 (or 81 normally)\n",
    "                epoch_cube_list_3D.append(im)\n",
    "\n",
    "# Saving files as .npy \n",
    "    np.save(f\"{dir_path_save}/3Darrays_{os.path.basename(os.path.normpath(participant))}_mycontrast.npy\", epoch_cube_list_3D, allow_pickle=True)            \n",
    "    np.save(f\"{dir_path_save}/labels_{os.path.basename(os.path.normpath(participant))}_mycontrast.npy\", epoch_cube_labels_3D, allow_pickle=True)  \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "325ab36649285f6ad34d2ff5a01d2d0a41558fa3d31820ecc8d37f5567909206"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
