{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fNIRS Preprocessing pipeline & conversion to GAF-images\n",
    "\n",
    "This document includes the code for the preprocessing of the collected fNIRS data using the MNE-nirs toolbox as well as the code for converting the segmented data into GAF images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the relevant packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/klarafomsgaard/opt/anaconda3/lib/python3.9/site-packages/nilearn/datasets/__init__.py:93: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n",
      "/Users/klarafomsgaard/opt/anaconda3/lib/python3.9/site-packages/nilearn/glm/__init__.py:55: FutureWarning: The nilearn.glm module is experimental. It may change in any future release of Nilearn.\n",
      "  warn('The nilearn.glm module is experimental. '\n"
     ]
    }
   ],
   "source": [
    "# Importing packages and libraries related to the MNE-nirs toolbox\n",
    "import mne as mne\n",
    "import mne_nirs\n",
    "from mne.preprocessing.nirs import beer_lambert_law, optical_density\n",
    "from mne.io import read_raw_snirf\n",
    "import mne_bids\n",
    "from mne.preprocessing.nirs import (optical_density,\n",
    "                                    temporal_derivative_distribution_repair)\n",
    "\n",
    "# Additional packages \n",
    "import pyvista\n",
    "import pyvistaqt\n",
    "import pandas as pd\n",
    "import numba\n",
    "import numpy as np\n",
    "from itertools import compress\n",
    "import os\n",
    "\n",
    "# Packages related to image conversion\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as im\n",
    "from matplotlib import cm\n",
    "from pyts.image import GramianAngularField\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline for fNIRS using MNE\n",
    "The pipeline is written as a function as this allows for looping over all participants, applying the same pipeline to all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preproc_mne(filepath):\n",
    "    \n",
    "    # -- LOADING IN THE DATA --\n",
    "    # Set pyvista to be the 3D backend\n",
    "    mne.viz.set_3d_backend(\"pyvista\") \n",
    "\n",
    "    # Find the .snirf file containing the fNIRS data for the participant\n",
    "    fname = filepath \n",
    "\n",
    "    # Getting the raw intensity \n",
    "    raw_intensity = mne.io.read_raw_snirf(fname, verbose = True, preload = True).load_data()\n",
    "    \n",
    "    # raw optical density\n",
    "    raw_od = mne.preprocessing.nirs.optical_density(raw_intensity)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # -- INVESTIGATION OF SCALP COUPLING INDEX --\n",
    "\n",
    "    # sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
    "    # raw_od.info['bads'] = list(compress(raw_od.ch_names, sci < 0.5))\n",
    "    # print(raw_od.info['bads'])\n",
    "    # fig,ax=plt.subplots()\n",
    "    # ax.hist(sci)\n",
    "    # ax.set(xlabel='Scalp Coupling Index', ylabel='Count',xlim=[0,1])\n",
    "\n",
    "    ## This is not active code, at it was used in an earlier stage to find bad channels (sci < 0.5) for all participants\n",
    "    ## All bad channels were noted and consistently bad channels across participants were filtered out using the following code:\n",
    "\n",
    "    raw_od.info['bads'].extend([\"S3_D4 760\",\"S3_D4 850\",\"S8_D3 760\",\"S8_D3 850\",\"S8_D7 760\",\"S8_D7 850\",\n",
    "    \"S11_D9 760\",\"S11_D9 850\",\"S11_D11 760\",\"S11_D11 850\",\"S13_D14 760\",\"S13_D14 850\",\"S13_D37 760\",\"S13_D37 850\",\n",
    "    \"S15_D14 760\",\"S15_D14 850\",\"S15_D15 760\",\"S15_D15 850\",\"S17_D19 760\",\"S17_D19 850\",\"S21_D18 760\",\"S21_D18 850\",\n",
    "    \"S25_D22 760\",\"S25_D22 850\",\"S25_D24 760\",\"S25_D24 850\",\"S26_D23 760\",\"S26_D23 850\",\"S26_D24 760\",\"S26_D24 850\",\"S26_D25 760\",\n",
    "    \"S26_D25 850\",\"S26_D26 760\",\"S26_D26 850\",\"S28_D27 760\",\"S28_D27 850\",\"S28_D29 760\",\"S28_D29 850\",\"S32_D29 760\",\"S32_D29 850\"])\n",
    "\n",
    "    bads_list = raw_od.info[\"bads\"]\n",
    "    raw_od_no_bad = raw_od.copy().drop_channels(bads_list)\n",
    "\n",
    "\n",
    "\n",
    "    # -- ARTIFACT DETECTION & CORRECTION --\n",
    "\n",
    "    # Short channel motion correction\n",
    "    od_corrected = mne_nirs.signal_enhancement.short_channel_regression(raw_od_no_bad)\n",
    "    \n",
    "    # Motion correction and baseline shift using TDDR\n",
    "    corrected_tddr = temporal_derivative_distribution_repair(od_corrected)\n",
    "\n",
    "\n",
    "\n",
    "    # -- CONVERSION TO HBO & HBR \n",
    "\n",
    "    # Converting to change in oxy- and deoxyhemoglobin levels using the modified Lambert Beer Law\n",
    "    raw_hemo = beer_lambert_law(corrected_tddr,ppf=0.1)\n",
    "\n",
    "    # Enhancement of negative correlation between HbO and HbR\n",
    "    raw_hemo = mne_nirs.signal_enhancement.enhance_negative_correlation(raw_hemo)\n",
    "\n",
    "    # Only include the long channels, as the short channels are for motion artifact detection\n",
    "    raw_hemo_long = mne_nirs.channels.get_long_channels(raw_hemo)\n",
    " \n",
    "    # Apply bandpass filter to data\n",
    "    raw_hemo_long = raw_hemo_long.filter(0.01,0.7,h_trans_bandwidth = 0.3, l_trans_bandwidth=0.005) \n",
    " \n",
    "\n",
    "    # -- INSPECTION OF CLEANED DATA --\n",
    "    plt.rcParams[\"figure.figsize\"]=(16,10)\n",
    "    raw_hemo_long.plot(n_channels=50, duration=100, show_scrollbars=False)\n",
    "\n",
    "    # -- DATA SEGMENTATION --\n",
    "    # Generate events for epoching \n",
    "    events,event_id = mne.events_from_annotations(raw_hemo_long)\n",
    "    \n",
    "    # Epoching the data (segmentation into trial relevant sections)\n",
    "    reject_criteria = dict(hbo=80e-5) # Set rejection criterion\n",
    "    tmin,tmax = -2,7 # Timeframe for epochs: 2 second before and 7 seconds after event onset\n",
    "    epochs = mne.Epochs(raw_hemo_long,events,event_id = event_id,\n",
    "                        tmin=tmin,tmax=tmax,preload=True, baseline = (None,0), \n",
    "                        reject=reject_criteria, reject_by_annotation=True)\n",
    "\n",
    "    return epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image conversion to GAF images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function used to create the filename for the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to append suffixes\n",
    "def append_suffix(nr,channels, filename, id_phase, epochnr, stim):\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    return \"{nr}-{channels}_{id_phase}_{epochnr}_{stim}{ext}\".format(nr = nr, channels=channels, id_phase = id_phase, stim=stim, epochnr = epochnr, ext=ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to create folders \n",
    "The function together create one folder for each participant where images for each epoch is saved in separate folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required package\n",
    "from os import mkdir\n",
    "\n",
    "def createfolder_part(participant):\n",
    "    dir_part = os.path.join(\"/path/to/where/we/want/to/save/the/images\",f\"{participant}\")\n",
    "    if not os.path.exists(dir_part):\n",
    "        os.mkdir(dir_part)\n",
    "    return dir_part\n",
    "\n",
    "def createfolder_epoch(dir_part,epochnr,stim):\n",
    "    dir_epoch = os.path.join(f\"{dir_part}\",f\"{epochnr}_{stim}\")\n",
    "    if not os.path.exists(dir_epoch):\n",
    "        os.mkdir(dir_epoch)\n",
    "    return dir_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function that converts the data to a GAF image per channel per epoch\n",
    "\n",
    "The function converts haemodynamic response signals for each epoch into a 2D GAF image with the dimensions 36x36 pixels (timepoints x timepoints). \n",
    "\n",
    "\n",
    "Input:\n",
    "\n",
    "*epoch_basestim*: The epoched and preprocessed heamodynamic response signals extracted from the raw fNIRS data\n",
    "\n",
    "*id_phase*: The participant id and phase - this is used to include in the filename of the image\n",
    "\n",
    "*hbo*: Decides whether the images should include hbr channels or only hbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def norm_channel(epoch_basestim,id_phase,hbo):\n",
    "\n",
    "    # Create a folder for the participant\n",
    "    dir_part = createfolder_part(id_phase)\n",
    "\n",
    "    # -- CONVERT DATA INTO DATAFRAME --\n",
    "    if hbo == True: # This is run if only the hbo channels are desired\n",
    "        df_channels_base = epoch_basestim.copy().pick(\"hbo\")\n",
    "        df_channels = df_channels_base.to_data_frame()\n",
    "\n",
    "    else: # Run if we want both the hbo and hbr channels\n",
    "        df_channels_base_hbo = epoch_basestim.copy().pick(\"hbo\") \n",
    "        df_channels_base_hbo_df = df_channels_base_hbo.to_data_frame()\n",
    "\n",
    "        df_channels_base_hbr = epoch_basestim.copy().pick(\"hbr\") \n",
    "        df_channels_base_hbr_df = df_channels_base_hbr.to_data_frame()\n",
    "\n",
    "        df_channels = df_channels_base_hbo_df.join(df_channels_base_hbr_df.iloc[:,3:])\n",
    "\n",
    "\n",
    "    # -- LOOPING OVER EACH EPOCH AND CONVERTING THE DATA FOR EACH CHANNEL TO GAF --\n",
    "    for epoch in np.nditer(df_channels['epoch'].unique()):\n",
    "        epochnr = epoch \n",
    "        \n",
    "        # Selecting only one epoch and prepare for image conversion\n",
    "        epochsubset = df_channels.loc[df_channels['epoch'] == epoch] \n",
    "        stim = epochsubset.iloc[1]['condition'] # Save the condition to use in the filename when saving the image \n",
    "        epoch_df_clean = epochsubset.drop([\"time\",\"condition\",\"epoch\"],axis=1).transpose() # Clean the dataframe and arrange to one column per channel\n",
    "        epoch_array = epoch_df_clean.to_numpy() \n",
    "\n",
    "        # Convert data from each channel into a GAF image\n",
    "        gaf = GramianAngularField()\n",
    "        image = gaf.transform(epoch_array)\n",
    "        plt.set_cmap('rainbow')\n",
    "        filename = \"epochs.png\" \n",
    "        nr = 1\n",
    "        for i in image:  \n",
    "            if hbo == True: # Save the images in different folders depending on whether they include hbr channels or not\n",
    "                fullname = append_suffix(nr,\"hbo\",filename,id_phase,epochnr,stim)\n",
    "                image_path = createfolder_epoch(dir_part,epochnr,stim)\n",
    "            else: \n",
    "                fullname = append_suffix(nr,\"all\",filename,id_phase,epochnr,stim)\n",
    "                image_path = createfolder_epoch(dir_part,epochnr,stim)\n",
    "            plt.imsave(f\"{image_path}/{fullname}\",i) \n",
    "            nr = nr+1\n",
    "        print(f\"Epoch{epochnr}, channel {i} saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the images including only HbO channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a list of participants\n",
    "dir_path = \"/path/to/.snirf-files\"\n",
    "import glob \n",
    "snirf_list = glob.glob(f\"{dir_path}/*\")\n",
    "if '.DS_Store' in snirf_list:\n",
    "    snirf_list.remove('.DS_Store')\n",
    "\n",
    "# Loop over all participants using the above created functions\n",
    "for i in snirf_list:\n",
    "    print(f\"loading {i}\")\n",
    "    epochs = preproc_mne(f\"{i}\")\n",
    "    id_phase = i.split(\"/\")[6].split(\".\")[0] # This needs adjusting depending on your own path\n",
    "    norm_channel(epochs,id_phase,True)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "325ab36649285f6ad34d2ff5a01d2d0a41558fa3d31820ecc8d37f5567909206"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
